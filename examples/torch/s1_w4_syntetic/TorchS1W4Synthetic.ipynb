{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "7364ad50",
            "metadata": {},
            "source": [
                "## Prerequisites\n",
                "- Nerlnet 1.6.0 is built (`./NerlnetBuild.sh --infra torch`).\n",
                "- Synthetic CSV is synced to `/tmp/nerlnet/data/NerlnetData-master/nerlnet/synthetic_norm/synthetic_full.csv`.\n",
                "- The example JSONs (`exp_torch_s1_w4.json`, `dc_torch_s1_w4.json`, `conn_torch_s1_w4.json`) remain inside `examples/torch/s1_w4_syntetic`."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "68a405b6",
            "metadata": {},
            "source": [
                "## 1. Generate the TorchScript model\n",
                "The following cell recreates the placeholder perceptron that matches the JSON metadata and writes it to `examples/torch/s1_w4_syntetic/models/synthetic_torch_example.pt`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "767e47c9",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "from typing import Sequence\n",
                "\n",
                "import torch\n",
                "\n",
                "MODEL_PATH = Path(\"examples/torch/s1_w4_syntetic/models/synthetic_torch_example.pt\").resolve()\n",
                "INPUT_SIZE = 5\n",
                "HIDDEN_SIZES: Sequence[int] = (30, 5)\n",
                "LABELS = 3\n",
                "SAMPLES = 2000\n",
                "EPOCHS = 400\n",
                "LEARNING_RATE = 0.005\n",
                "SEED = 2025\n",
                "\n",
                "\n",
                "def generate_dataset(num_samples: int, num_features: int, num_labels: int, seed: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
                "    generator = torch.Generator().manual_seed(seed)\n",
                "    features = torch.rand((num_samples, num_features), generator=generator) * 10.0\n",
                "    weight = torch.randn((num_features, num_labels), generator=generator)\n",
                "    bias = torch.randn(num_labels, generator=generator)\n",
                "    logits = features @ weight + bias\n",
                "    labels = torch.softmax(logits, dim=1)\n",
                "    return features, labels\n",
                "\n",
                "\n",
                "def build_model(input_size: int, hidden_sizes: Sequence[int], output_size: int) -> torch.nn.Module:\n",
                "    layers = []\n",
                "    in_dim = input_size\n",
                "    for hidden in hidden_sizes:\n",
                "        layers.append(torch.nn.Linear(in_dim, hidden))\n",
                "        layers.append(torch.nn.ReLU())\n",
                "        in_dim = hidden\n",
                "    layers.append(torch.nn.Linear(in_dim, output_size))\n",
                "    return torch.nn.Sequential(*layers)\n",
                "\n",
                "\n",
                "class FeatureSliceModule(torch.nn.Module):\n",
                "    def __init__(self, backbone: torch.nn.Module, feature_width: int) -> None:\n",
                "        super().__init__()\n",
                "        self.backbone = backbone\n",
                "        self.feature_width = feature_width\n",
                "\n",
                "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
                "        if inputs.size(-1) < self.feature_width:\n",
                "            raise RuntimeError(\n",
                "                f\"Expected at least {self.feature_width} columns, received {inputs.size(-1)}\"\n",
                "            )\n",
                "        features_only = inputs[..., : self.feature_width]\n",
                "        return self.backbone(features_only)\n",
                "\n",
                "\n",
                "def train_model(model: torch.nn.Module, features: torch.Tensor, labels: torch.Tensor, epochs: int, lr: float) -> None:\n",
                "    criterion = torch.nn.MSELoss()\n",
                "    for _ in range(max(1, epochs)):\n",
                "        preds = model(features)\n",
                "        loss = criterion(preds, labels)\n",
                "        loss.backward()\n",
                "        with torch.no_grad():\n",
                "            for param in model.parameters():\n",
                "                if param.grad is None:\n",
                "                    continue\n",
                "                param -= lr * param.grad\n",
                "                param.grad.zero_()\n",
                "\n",
                "\n",
                "torch.manual_seed(SEED)\n",
                "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
                "features, labels = generate_dataset(SAMPLES, INPUT_SIZE, LABELS, SEED)\n",
                "model = build_model(INPUT_SIZE, HIDDEN_SIZES, LABELS)\n",
                "train_model(model, features, labels, EPOCHS, LEARNING_RATE)\n",
                "model.eval()\n",
                "scripted = torch.jit.script(FeatureSliceModule(model, INPUT_SIZE))\n",
                "scripted.save(str(MODEL_PATH))\n",
                "print(f\"[TorchS1W4] Wrote TorchScript model to {MODEL_PATH}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8371ba35",
            "metadata": {},
            "source": [
                "## 2. Inspect the distributed configuration\n",
                "This helper cell visualizes the Torch worker definition and confirms the per-batch metadata that the worker expects during training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "862d2fab",
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "from pprint import pprint\n",
                "\n",
                "with open(\"examples/torch/s1_w4_syntetic/dc_torch_s1_w4.json\", \"r\", encoding=\"utf-8\") as handle:\n",
                "    dc = json.load(handle)\n",
                "workers = dc[\"workers\"]\n",
                "train_params = dc[\"model_sha\"][\"torch_s1w4_v160\"][\"train_params\"]\n",
                "print(\"Workers bound to the example model:\")\n",
                "pprint(workers)\n",
                "print(\"\\nTorch training parameters:\")\n",
                "pprint(train_params)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cd6b2a31",
            "metadata": {},
            "source": [
                "## 3. Stage the JSONs and launch Nerlnet\n",
                "1. Copy or symlink the JSON files into the paths referenced by `config/jsonsDir.nerlconfig` and `config/subnets.nerlconfig` (for example, point `dc.json` to `examples/torch/s1_w4_syntetic/dc_torch_s1_w4.json`).\n",
                "2. Ensure `conn.json` resolves to `examples/torch/s1_w4_syntetic/conn_torch_s1_w4.json` and the experiment description is reachable at `examples/torch/s1_w4_syntetic/exp_torch_s1_w4.json`.\n",
                "3. Build or rebuild the native bridges if anything changed (`./NerlnetBuild.sh --infra all`).\n",
                "4. Launch the Erlang application via `./NerlnetRun.sh` in one terminal and keep it running."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "35faaa58",
            "metadata": {},
            "source": [
                "## 4. Kick off the Torch full-flow\n",
                "With Nerlnet running, trigger the example from a second terminal:\n",
                "```bash\n",
                "tests/NerlnetFullFlowTorchTest.sh --manual-start \\\n",
                "  --experiment examples/torch/s1_w4_syntetic/exp_torch_s1_w4.json \\\n",
                "  --dc examples/torch/s1_w4_syntetic/dc_torch_s1_w4.json \\\n",
                "  --conn examples/torch/s1_w4_syntetic/conn_torch_s1_w4.json\n",
                "```\n",
                "As batches stream through `s1`, each worker (`w1`-`w4`) stays active on the Torch bridge, preserving optimizer momentum and weights between messages.\n",
                "Once both training and prediction phases finish, inspect `/tmp/nerlnet/results/` for anomaly summaries and `/tmp/nerlnet/torch/models/pt/` for updated checkpoints."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b5130be6",
            "metadata": {},
            "source": [
                "## 5. Analyze outcomes\n",
                "Review `test_torch.log` (or the CSVs saved under `/tmp/nerlnet/results/.../model_perf.csv`) to compare Torch metrics against the OpenNN baseline. If the anomaly scores diverge, double-check that the generated TorchScript file matches the shapes advertised in `train_params` and that the JSON paths inside `config/jsonsDir.nerlconfig` are up to date."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}